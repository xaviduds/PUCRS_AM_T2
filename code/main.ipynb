{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import  YOLO\n",
    "# transfear learning ADAPT[30] (Python), TLlib [31] (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model. Sizes: n, s, m, l, x; -seg is for segmentation\n",
    "model = YOLO(\"../models/yolov8n-seg.pt\")  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.109 ðŸš€ Python-3.11.3 torch-2.0.1+cu117 CPU\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=../2 - models/yolov8n-seg.pt, data=coco128-seg.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n",
      "YOLOv8n-seg summary: 261 layers, 3409968 parameters, 3409952 gradients\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/eduardo/Documents/GitHub/data/experiments/S000_yolov8/datasets/coco128-seg/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/eduardo/Documents/GitHub/data/experiments/S000_yolov8/datasets/coco128-seg/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G      1.021      2.355      1.346      1.117        207        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:43<00:00,  5.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:16<00:00,  4.10s/it]\n",
      "                   all        128        929      0.551      0.585      0.607      0.457      0.519      0.549      0.565      0.373\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G      1.134      2.402      1.388      1.183        145        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:42<00:00,  5.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:16<00:00,  4.07s/it]\n",
      "                   all        128        929      0.617      0.589      0.623      0.469      0.591      0.563      0.587      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G       1.12       2.34      1.276      1.155        208        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:43<00:00,  5.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.83s/it]\n",
      "                   all        128        929      0.633      0.612       0.64      0.479      0.586      0.574      0.597      0.395\n",
      "\n",
      "3 epochs completed in 0.052 hours.\n",
      "Optimizer stripped from runs/segment/train/weights/last.pt, 7.1MB\n",
      "Optimizer stripped from runs/segment/train/weights/best.pt, 7.1MB\n",
      "\n",
      "Validating runs/segment/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.109 ðŸš€ Python-3.11.3 torch-2.0.1+cu117 CPU\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3404320 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.79s/it]\n",
      "                   all        128        929      0.635      0.609       0.64       0.48      0.587      0.575      0.597      0.395\n",
      "                person        128        254      0.815      0.693      0.788      0.559      0.765      0.669      0.739      0.396\n",
      "               bicycle        128          6       0.51      0.167      0.419      0.246      0.505      0.167      0.441      0.244\n",
      "                   car        128         46      0.544      0.239       0.31       0.17      0.491      0.217       0.26      0.113\n",
      "            motorcycle        128          5      0.823      0.937      0.962      0.764      0.826      0.957      0.962      0.554\n",
      "              airplane        128          6      0.747      0.987      0.955      0.797      0.622      0.825      0.851      0.567\n",
      "                   bus        128          7      0.657      0.714      0.733      0.641      0.656      0.714      0.722      0.591\n",
      "                 train        128          3      0.538          1      0.913      0.629      0.536          1      0.913      0.606\n",
      "                 truck        128         12          1      0.302      0.485      0.231          1      0.309      0.538      0.266\n",
      "                  boat        128          6      0.581        0.5      0.523      0.359      0.378      0.333      0.386      0.138\n",
      "         traffic light        128         14      0.758      0.214      0.229      0.145      0.489      0.143      0.164      0.143\n",
      "             stop sign        128          2          1      0.886      0.995      0.747          1      0.889      0.995      0.846\n",
      "                 bench        128          9      0.583      0.314      0.504      0.219      0.207      0.111      0.209      0.096\n",
      "                  bird        128         16       0.96      0.875      0.955      0.677      0.949      0.875      0.955      0.487\n",
      "                   cat        128          4      0.658          1      0.995      0.871      0.653          1      0.995      0.846\n",
      "                   dog        128          9      0.812      0.778      0.841      0.654      0.809      0.778      0.808      0.539\n",
      "                 horse        128          2      0.677          1      0.995      0.659      0.337        0.5       0.62      0.236\n",
      "              elephant        128         17          1      0.933      0.946      0.692      0.944      0.882      0.888      0.608\n",
      "                  bear        128          1       0.61          1      0.995      0.995      0.606          1      0.995      0.995\n",
      "                 zebra        128          4       0.85          1      0.995      0.971      0.848          1      0.995      0.853\n",
      "               giraffe        128          9       0.96          1      0.995      0.701      0.741      0.778      0.791      0.467\n",
      "              backpack        128          6      0.778      0.333      0.533        0.3      0.764      0.333      0.457      0.296\n",
      "              umbrella        128         18      0.546      0.778      0.722      0.505      0.459      0.667      0.547       0.23\n",
      "               handbag        128         19          1      0.133      0.285      0.203          1      0.139       0.29      0.139\n",
      "                   tie        128          7      0.665      0.568      0.612      0.441      0.667      0.571      0.612      0.375\n",
      "              suitcase        128          4      0.459      0.858       0.87      0.652       0.49      0.966       0.87      0.583\n",
      "               frisbee        128          5      0.665        0.8      0.732      0.656      0.659        0.8      0.732      0.487\n",
      "                  skis        128          1       0.46          1      0.995      0.697      0.455          1      0.995      0.101\n",
      "             snowboard        128          7      0.686      0.857      0.858      0.612      0.227      0.286      0.302      0.198\n",
      "           sports ball        128          6       0.57      0.333      0.475       0.27      0.283      0.167      0.311      0.258\n",
      "                  kite        128         10      0.447        0.3      0.406      0.188      0.293        0.2      0.229      0.126\n",
      "          baseball bat        128          4          1          0      0.412     0.0845          1      0.283      0.678      0.232\n",
      "        baseball glove        128          7      0.778      0.429       0.43      0.274       0.77      0.429      0.449       0.31\n",
      "            skateboard        128          5      0.535        0.2      0.349      0.255      0.528        0.2      0.352      0.174\n",
      "         tennis racket        128          7      0.534      0.429      0.415      0.273       0.53      0.429      0.471      0.272\n",
      "                bottle        128         18      0.447      0.333      0.408      0.225      0.443      0.333      0.381       0.23\n",
      "            wine glass        128         16      0.534       0.75      0.645      0.331      0.354        0.5      0.361      0.231\n",
      "                   cup        128         36      0.631      0.333      0.408      0.304      0.621      0.333      0.407      0.282\n",
      "                  fork        128          6      0.364      0.167      0.225      0.196      0.355      0.167      0.225      0.128\n",
      "                 knife        128         16      0.604      0.438      0.567      0.384      0.578      0.438      0.465      0.334\n",
      "                 spoon        128         22      0.517      0.342      0.407      0.265      0.533      0.364      0.389       0.18\n",
      "                  bowl        128         28      0.649       0.75      0.737      0.595      0.647       0.75       0.72      0.422\n",
      "                banana        128          1      0.135          1      0.249      0.174      0.134          1      0.249      0.134\n",
      "              sandwich        128          2      0.476          1      0.745      0.696      0.468          1      0.745      0.671\n",
      "                orange        128          4          1      0.418      0.808      0.535          1      0.437      0.808      0.463\n",
      "              broccoli        128         11      0.482      0.256      0.326      0.247      0.488      0.262      0.362      0.256\n",
      "                carrot        128         24      0.544      0.583      0.643      0.442      0.539      0.583      0.639      0.406\n",
      "               hot dog        128          2      0.557          1      0.995      0.995      0.555          1      0.995      0.945\n",
      "                 pizza        128          5       0.58          1       0.92      0.802      0.577          1       0.92      0.762\n",
      "                 donut        128         14      0.602      0.972      0.934      0.848      0.603      0.978      0.934      0.754\n",
      "                  cake        128          4      0.682          1      0.995      0.945      0.678          1      0.995      0.833\n",
      "                 chair        128         35       0.48      0.528      0.506      0.265       0.49      0.543      0.458      0.207\n",
      "                 couch        128          6      0.661      0.833      0.788      0.654      0.521      0.667      0.647       0.41\n",
      "          potted plant        128         14      0.687      0.571      0.668      0.439      0.762      0.643      0.698      0.367\n",
      "                   bed        128          3      0.969          1      0.995      0.659      0.954          1      0.995      0.515\n",
      "          dining table        128         13      0.496      0.538      0.506      0.394       0.14      0.154      0.135     0.0749\n",
      "                toilet        128          2      0.498        0.5      0.516      0.511      0.488        0.5      0.516      0.507\n",
      "                    tv        128          2        0.6          1      0.995      0.895      0.588          1      0.995      0.945\n",
      "                laptop        128          3       0.53      0.333      0.516      0.491      0.516      0.333      0.503      0.309\n",
      "                 mouse        128          2          0          0     0.0711     0.0254          0          0     0.0223     0.0134\n",
      "                remote        128          8      0.823      0.625      0.647       0.53      0.815      0.625       0.66      0.467\n",
      "            cell phone        128          8      0.505      0.129      0.187     0.0936      0.521      0.141      0.179     0.0931\n",
      "             microwave        128          3       0.57      0.667      0.863      0.686       0.56      0.667      0.863      0.641\n",
      "                  oven        128          5      0.433        0.4      0.348      0.274      0.643        0.6      0.619      0.398\n",
      "                  sink        128          6      0.278      0.167      0.308      0.212      0.275      0.167      0.316      0.218\n",
      "          refrigerator        128          5      0.732        0.8      0.732      0.516      0.725        0.8      0.732      0.515\n",
      "                  book        128         29      0.428      0.207      0.373      0.174      0.345      0.172       0.24     0.0932\n",
      "                 clock        128          9      0.792      0.851      0.852      0.713      0.797      0.874      0.852      0.679\n",
      "                  vase        128          2      0.279          1      0.663      0.597      0.267          1      0.663      0.531\n",
      "              scissors        128          1          1          0          0          0          1          0          0          0\n",
      "            teddy bear        128         21       0.64      0.476      0.604      0.372      0.564      0.432      0.509      0.287\n",
      "            toothbrush        128          5      0.648      0.743      0.658      0.428      0.652      0.755      0.658      0.366\n",
      "Speed: 1.0ms preprocess, 79.8ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Trains the model, creates the 'runs' folder\n",
    "model.train(data='coco128-seg.yaml', epochs=3, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.109 ðŸš€ Python-3.11.3 torch-2.0.1+cu117 CPU\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3404320 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/eduardo/Documents/GitHub/data/experiments/S000_yolov8/datasets/coco128-seg/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:13<00:00,  1.73s/it]\n",
      "                   all        128        929      0.623      0.602      0.639      0.477      0.578      0.563      0.594      0.392\n",
      "                person        128        254      0.832      0.689      0.789      0.561      0.788      0.657       0.74        0.4\n",
      "               bicycle        128          6      0.446      0.167      0.401      0.243      0.433      0.167      0.408       0.23\n",
      "                   car        128         46      0.614      0.239      0.295      0.161      0.553      0.217      0.246      0.108\n",
      "            motorcycle        128          5      0.827       0.96      0.962      0.764      0.827      0.964      0.962      0.554\n",
      "              airplane        128          6      0.745      0.976      0.955      0.797      0.618      0.811      0.851      0.567\n",
      "                   bus        128          7      0.612      0.714      0.732      0.641       0.61      0.714      0.722      0.591\n",
      "                 train        128          3      0.544          1      0.913      0.629      0.543          1      0.913      0.606\n",
      "                 truck        128         12      0.945      0.333      0.477      0.257       0.94      0.333      0.533      0.235\n",
      "                  boat        128          6      0.565        0.5      0.565      0.337      0.374      0.333      0.406      0.144\n",
      "         traffic light        128         14      0.775      0.214      0.235      0.148      0.514      0.143      0.169      0.145\n",
      "             stop sign        128          2          1      0.878      0.995      0.747          1       0.88      0.995      0.846\n",
      "                 bench        128          9      0.576      0.307      0.504      0.218      0.211      0.111      0.208     0.0958\n",
      "                  bird        128         16      0.988      0.875      0.956      0.678      0.983      0.875      0.956      0.487\n",
      "                   cat        128          4      0.667          1      0.995      0.871      0.666          1      0.995      0.846\n",
      "                   dog        128          9      0.819      0.778      0.846      0.656      0.818      0.778      0.811      0.542\n",
      "                 horse        128          2      0.558          1      0.995      0.657      0.276        0.5      0.606      0.232\n",
      "              elephant        128         17          1      0.928      0.946      0.692      0.951      0.882      0.888      0.608\n",
      "                  bear        128          1      0.618          1      0.995      0.995      0.616          1      0.995      0.995\n",
      "                 zebra        128          4      0.854          1      0.995      0.946      0.853          1      0.995      0.853\n",
      "               giraffe        128          9      0.865          1      0.984      0.712      0.713      0.833      0.828      0.464\n",
      "              backpack        128          6      0.811      0.333      0.533      0.331      0.804      0.333      0.457      0.296\n",
      "              umbrella        128         18       0.57      0.778       0.72      0.504      0.485      0.667      0.546       0.23\n",
      "               handbag        128         19          1      0.119      0.284      0.202          1      0.122      0.257      0.136\n",
      "                   tie        128          7      0.662      0.561      0.613      0.441      0.662      0.562      0.613      0.375\n",
      "              suitcase        128          4      0.503       0.75       0.87      0.652      0.497       0.75       0.87      0.583\n",
      "               frisbee        128          5       0.65      0.751      0.683      0.616      0.654      0.763      0.683      0.453\n",
      "                  skis        128          1      0.472          1      0.995      0.697       0.47          1      0.995      0.101\n",
      "             snowboard        128          7        0.7      0.857      0.858      0.611      0.232      0.286      0.304      0.197\n",
      "           sports ball        128          6      0.577      0.333      0.471      0.265      0.288      0.167      0.311      0.257\n",
      "                  kite        128         10      0.475        0.3      0.405      0.187      0.315        0.2      0.226      0.125\n",
      "          baseball bat        128          4     0.0657     0.0329      0.274     0.0669      0.146     0.0729      0.504      0.166\n",
      "        baseball glove        128          7      0.803      0.429       0.43      0.275      0.799      0.429       0.45      0.311\n",
      "            skateboard        128          5      0.573        0.2      0.381      0.275       0.57        0.2      0.381      0.219\n",
      "         tennis racket        128          7      0.544      0.429      0.417      0.287      0.542      0.429      0.473      0.266\n",
      "                bottle        128         18      0.445      0.357      0.395      0.233      0.448      0.361      0.363      0.242\n",
      "            wine glass        128         16      0.503       0.75      0.735      0.339      0.251      0.375      0.337      0.216\n",
      "                   cup        128         36      0.674      0.333      0.422      0.317      0.673      0.333      0.394      0.273\n",
      "                  fork        128          6      0.318      0.167      0.236      0.201      0.316      0.167      0.236      0.131\n",
      "                 knife        128         16      0.646      0.438       0.55      0.382      0.643      0.438      0.454      0.325\n",
      "                 spoon        128         22      0.542      0.318      0.398      0.259      0.528      0.364      0.389      0.174\n",
      "                  bowl        128         28      0.649       0.75      0.748      0.603      0.646       0.75      0.727      0.413\n",
      "                banana        128          1      0.147          1      0.199      0.179      0.145          1      0.199      0.139\n",
      "              sandwich        128          2       0.65          1      0.828      0.795      0.621          1      0.828      0.729\n",
      "                orange        128          4          1      0.373      0.808      0.535          1      0.382      0.808      0.463\n",
      "              broccoli        128         11      0.315      0.182      0.295      0.224      0.314      0.182      0.312      0.235\n",
      "                carrot        128         24      0.571      0.583      0.609      0.421      0.602      0.625       0.63      0.404\n",
      "               hot dog        128          2      0.605          1      0.995      0.995      0.603          1      0.995      0.895\n",
      "                 pizza        128          5      0.612          1      0.962      0.817      0.609          1      0.962       0.78\n",
      "                 donut        128         14      0.599          1      0.936      0.849      0.598          1      0.936      0.753\n",
      "                  cake        128          4       0.69          1      0.995        0.9      0.689          1      0.995      0.833\n",
      "                 chair        128         35      0.474      0.514      0.495      0.264      0.492      0.543       0.46      0.211\n",
      "                 couch        128          6      0.685      0.833      0.789      0.654      0.544      0.667      0.647       0.41\n",
      "          potted plant        128         14       0.71      0.571      0.668      0.439      0.793      0.643      0.699      0.368\n",
      "                   bed        128          3          1      0.983      0.995      0.539          1          1      0.995      0.472\n",
      "          dining table        128         13      0.568      0.538      0.531      0.404      0.159      0.154       0.14     0.0801\n",
      "                toilet        128          2       0.51        0.5      0.516      0.511      0.508        0.5      0.516      0.508\n",
      "                    tv        128          2       0.63          1      0.995      0.895      0.624          1      0.995      0.945\n",
      "                laptop        128          3      0.495      0.333        0.5      0.477      0.493      0.333      0.491      0.302\n",
      "                 mouse        128          2          0          0     0.0852     0.0281          0          0     0.0222     0.0133\n",
      "                remote        128          8      0.858      0.625       0.64      0.527      0.846      0.625      0.652      0.463\n",
      "            cell phone        128          8      0.431      0.108      0.187     0.0938      0.447      0.112      0.179     0.0932\n",
      "             microwave        128          3      0.565      0.667      0.863      0.594      0.558      0.667      0.863      0.602\n",
      "                  oven        128          5      0.444        0.4      0.348      0.254      0.663        0.6      0.619      0.398\n",
      "                  sink        128          6      0.283      0.167      0.355      0.254      0.281      0.167      0.364       0.26\n",
      "          refrigerator        128          5      0.751        0.8      0.772      0.561      0.747        0.8      0.772      0.553\n",
      "                  book        128         29      0.444      0.172      0.371      0.175      0.386      0.153      0.285      0.101\n",
      "                 clock        128          9      0.781      0.793      0.852      0.713      0.783      0.805      0.852      0.679\n",
      "                  vase        128          2      0.304          1      0.663      0.597      0.299          1      0.663      0.531\n",
      "              scissors        128          1          1          0          0          0          1          0          0          0\n",
      "            teddy bear        128         21      0.706      0.476      0.603      0.371      0.614      0.429      0.509      0.287\n",
      "            toothbrush        128          5      0.393        0.6       0.57      0.345      0.391        0.6       0.57      0.296\n",
      "Speed: 0.9ms preprocess, 75.4ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluates the model, creates the 'runs/segment/val' folder\n",
    "metrics = model.val()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_094925.jpg: 640x288 1 bottle, 1 vase, 41.8ms\n",
      "image 2/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_094928.jpg: 640x288 1 bottle, 1 vase, 45.1ms\n",
      "image 3/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_095007.jpg: 640x288 1 bottle, 30.6ms\n",
      "image 4/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_095028.jpg: 640x288 1 vase, 30.5ms\n",
      "image 5/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_095047.jpg: 640x288 1 vase, 32.2ms\n",
      "image 6/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_095103.jpg: 640x288 1 bottle, 1 vase, 29.2ms\n",
      "image 7/10 /home/eduardo/Documents/GitHub/pucrs_am_t2/1 - data/D001_papel_atras/20230526_095122.jpg: 640x288 1 bottle, 1 vase, 45.2ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Uses photos for the prediction/segmentation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m photo \u001b[39min\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m../1 - data/D001_papel_atras/\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     model(\u001b[39m'\u001b[39;49m\u001b[39m../1 - data/D001_papel_atras/\u001b[39;49m\u001b[39m'\u001b[39;49m, save\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:111\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model))\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/utils/_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m             \u001b[39m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     55\u001b[0m             \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 56\u001b[0m                 response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(request)\n\u001b[1;32m     58\u001b[0m \u001b[39m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m     \u001b[39m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 240\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:314\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    311\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/nn/tasks.py:213\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/nn/tasks.py:64\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 64\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     65\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:183\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    181\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    182\u001b[0m y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv2(torch\u001b[39m.\u001b[39;49mcat(y, \u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:43\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     42\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Uses photos for the prediction/segmentation\n",
    "for photo in '../data/D001_papel_atras/':\n",
    "    model('../data/D001_papel_atras/', save=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.109 ðŸš€ Python-3.11.3 torch-2.0.1+cu117 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/segment/train/weights/best.pt with input shape (16, 3, 640, 640) BCHW and output shape(s) ((16, 116, 8400), (16, 32, 160, 160)) (6.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.6s, saved as runs/segment/train/weights/best.onnx (13.2 MB)\n",
      "\n",
      "Export complete (3.8s)\n",
      "Results saved to \u001b[1m/home/eduardo/Documents/GitHub/pucrs_am_t2/3 - code/runs/segment/train/weights\u001b[0m\n",
      "Predict:         yolo predict task=segment model=runs/segment/train/weights/best.onnx imgsz=640 \n",
      "Validate:        yolo val task=segment model=runs/segment/train/weights/best.onnx imgsz=640 data=/home/eduardo/anaconda3/envs/machinelearning/lib/python3.11/site-packages/ultralytics/datasets/coco128-seg.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
